{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Toulouse public library dataset analysis.\n",
    "\n",
    "> Test your data analysis skills with some real world data processing problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#mojibake #datamisalignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A photograph of the Toulouse public Library](img/bibliothèque_toulouse.gif \"Photographie de la médiathèque José Cabanis, Toulouse (Source: Toulouse Métropole).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the shelves of Toulouse's public libraries, this notebook captures a snapshot of its customer habits in an unusual fashion. Whether you're a data enthusiast, a library lover, or simply curious about the urban cultural tastes of the pink city, you're about to follow a data trail and jump into a rabbit hole made of musical, cinematic and literary explorations.\n",
    "\n",
    "Beware: the raw datasets are flawed, making the path to a usable dataset bumpy. We'll need to address several issues as we go."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The full datasets are available on the [Toulouse Metropolis open data portal](https://data.toulouse-metropole.fr/pages/accueil/) and in this repository for reproducibility purposes.\n",
    "\n",
    "- [top 500 books loans](https://data.toulouse-metropole.fr/explore/dataset/top-500-des-imprimes-les-plus-empruntes-a-la-bibliotheque-de-toulouse/information/)\n",
    "- [top 500 movies loans](https://data.toulouse-metropole.fr/explore/dataset/top-500-des-films-les-plus-empruntes-a-la-bibliotheque-de-toulouse/information/)\n",
    "- [top 500 songs loans](https://data.toulouse-metropole.fr/explore/dataset/top-500-des-cds-les-plus-empruntes-a-la-bibliotheque-de-toulouse/information/)\n",
    "\n",
    "They are provided under a `Licence Ouverte v2.0 (Etalab)` [license](https://www.etalab.gouv.fr/wp-content/uploads/2017/04/ETALAB-Licence-Ouverte-v2.0.pdf).\n",
    "\n",
    "> I am not young enough to know everything.\n",
    "> \n",
    "> James M. Barrie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "  Toulouse public library dataset analysis. \n",
    "- [Part I – A dataset that needs some patching](#toulouse-public-library-dataset-analysis-part-I--a-dataset-that-(definately)-needs-some-patching)\n",
    "  - [Steps](#steps)\n",
    "  - [Requirements](#requirements)\n",
    "  - [Imports](#imports)\n",
    "  - [Configuration](#configuration)\n",
    "  - [Utilities](#utilities)\n",
    "    - [A quest for the first matching path](#a-quest-for-the-first-matching-path)\n",
    "    - [Why encoding detection matters](#why-encoding-detection-matters)\n",
    "    - [Broken alignments: fixing the lines](#broken-alignments-fixing-the-lines)\n",
    "- [Part II – Data analysis](#part-ii--data-analysis)\n",
    "  - [The LoanAnalysis Class: overview](#the-loananalyis-class-overview)\n",
    "  - [Trending and shooting star logics](#trending-and-shooting-star-logics)\n",
    "  - [Main orchestration](#main-orchestration)\n",
    "- [References](#references)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I - A dataset that (definately) needs some patching\n",
    "\n",
    "The dataset consists of 3 homogeneous CSV files (with identical column structures). One per category:\n",
    "- Prints\n",
    "- Movies\n",
    "- Music"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Column name   | Type    | Description                     | Example      | \n",
    "|---------------|---------|---------------------------------|--------------|\n",
    "| ANNEE         | Integer | Year of the record              | 2019         | \n",
    "| Nbre de prêts | Integer | Number of loans                 | 93           |\n",
    "| TITRE         | String  | Title of the work               | Enfantillages|\n",
    "| AUTEUR        | String  | Author                          | Aldebert     | \n",
    "| Editeur       | String  | Publisher                       | Skyzo Music | \n",
    "| Indice        | String  | Index                 | S099.2            | \n",
    "| BIB           | String  | Library code                    | CABANIS       | \n",
    "| COTE          | String  | Location label    | E 780.2 ALD     | \n",
    "| Cat 1         | String  | Category label 1 (Audience)                | E       | \n",
    "| Cat 2         | String  | Category label 2    (Media type)            | CD          | \n",
    "\n",
    "- “–” represents missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps\n",
    "\n",
    "**Data processing:**\n",
    "- Load CSV files matching discriminators (e.g. contains the \"*imprimes*\" (prints) keyword).\n",
    "- Detect file encoding and fix *mojibake* characters.\n",
    "- Repair line-level misalignment.\n",
    "\n",
    "**Data analysis:** \n",
    "- Aggregate and compute basic insights: top-N, trends (progressions/regressions), and sudden drops in popularity.\n",
    "- Exports insights in per-discriminator named JSON outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements\n",
    "- It is recommended to use Python 3.10+ with jupyterlab 4.4.4+\n",
    "- If not already in your environment, install pandas and chardet used in the notebook (uncomment the cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run once if packages aren't installed\n",
    "#!pip install pandas chardet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Mapping, Dict, Iterable, List, Optional\n",
    "import os\n",
    "from io import StringIO\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import chardet\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A config dataclass is a Python configuration object using the `@dataclass` decorator to create an immutable (frozen) configuration container for storing settings and parameters. The `Config` class contains keywords to load the expected files and column names from the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(frozen=True)\n",
    "class Config:\n",
    "    dataset_folder: str = \"dataset\"\n",
    "    output_folder: str = \"output\"\n",
    "    discriminator: str = \"imprimes\"\n",
    "    discriminator_list: Tuple[str, ...] = (\"films\", \"imprimes\", \"cds\")\n",
    "    top_n: int = 10\n",
    "    output_json: str = \"\"\n",
    "    year_field: str = \"ANNEE\"\n",
    "    popularity_field: str = \"Nbre de prêts\"\n",
    "    title_field: str = \"TITRE\"\n",
    "    author_field: str = \"AUTEUR\"\n",
    "    cat1_field: str = \"Cat 1\"\n",
    "    cat1_selector: Tuple[str, ...] = (\"A\", \"E\")\n",
    "    composite_field: str = \"Item_ID\"\n",
    "    replacements: Mapping[str, str] = None\n",
    "\n",
    "    def __post_init__(self):\n",
    "        object.__setattr__(self, \"output_json\", f\"results_{self.discriminator}.json\")\n",
    "        if self.replacements is None:\n",
    "            object.__setattr__(self, \"replacements\", {\n",
    "                # Empiric replacements observed in some mojibake cases\n",
    "                'ãa': 'â', 'ãe': 'ê', 'ãi': 'î', 'ão': 'ô',\n",
    "                'ãu': 'û', 'âe': 'é', 'áa': 'à', 'áe': 'è',\n",
    "                'ðc': 'ç'\n",
    "            })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About the **replacements** mapping: \n",
    "\n",
    "This is a pragmatic mapping (where both keys and values are strings) for the garbled digraphs observed in the dataset. In this case, it's better to have a *heuristic* approach: inspect real errors and add targeted mappings rather than a general brute-force transform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A quest for the first matching path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the datasets, we need to locate the correct files using wildcard patterns and ensure at least one matching file exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_matching_file(pattern: str) -> str:\n",
    "    \"\"\"Return the first file matching glob pattern or raise FileNotFoundError.\"\"\"\n",
    "    matches = glob.glob(pattern)\n",
    "    if not matches:\n",
    "        raise FileNotFoundError(f\"No file matching pattern: {pattern!r}\")\n",
    "    return matches[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why encoding detection matters\n",
    "\n",
    "While inspecting the dataset, I ran into encoding issues, these are often overlooked by teachers. Most course's datasets are clean, leading to a dismissal of the problem as lacking real Computer Science value. It's important to be aware of such issues: you will encounter them.\n",
    "\n",
    "**In short:** \n",
    "If you open bytes with the wrong encoding, you get garbled text where accented characters become sequences like `Ã©` instead of `é`. This is commonly refered to as [mojibake](https://en.wikipedia.org/wiki/Mojibake).\n",
    "\n",
    "> Example: the two-byte UTF-8 encoding for `é` is `0xC3` `0xA9`. If these bytes are interpreted as Latin‑1, they become `Ã` (0xC3) followed by `©` (0xA9) — rendering as `Ã©`.\n",
    "\n",
    "Character encodings map bytes to characters. CSV files produced by different systems (Windows, Mac, Linux; different locales) may use different encodings (*UTF-8, ISO-8859-1 a.k.a. Latin-1, Windows-1252, CP850, etc.*) that are not interoperable.\n",
    "\n",
    "Detecting and handling encoding is important when:\n",
    "- The CSV contains accented characters (French, Spanish, Portuguese..).\n",
    "- The CSV is exported from legacy systems (often Windows‑1252 or ISO‑8859‑1).\n",
    "- You need to preserve text for grouping/joins (e.g., the same title must match).\n",
    "\n",
    "[chardet](https://pypi.org/project/chardet/) guesses the encoding from a byte sample and open the file using that guess. chardet is heuristic — it often guesses correctly but not always. If you already know the encoding from source, prefer that.\n",
    "\n",
    "*References:*\n",
    "- Explainer on [encoding issues](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sniff_encoding_and_show(filepath: str, sample_bytes: int = 200):\n",
    "    \"\"\"Show raw bytes and decode attempts to help choose the correct encoding.\"\"\"\n",
    "    dataset_folder = Config.dataset_folder\n",
    "    filepath = os.path.join(dataset_folder, filepath)\n",
    "    b = Path(filepath).read_bytes()[:sample_bytes]\n",
    "    print(\"Raw bytes (hex):\", b.hex())\n",
    "    guess = chardet.detect(b)\n",
    "    print(\"chardet guess:\", guess)\n",
    "    for enc in (\"utf-8\", \"windows-1252\", \"iso-8859-1\", \"cp850\"):\n",
    "        try:\n",
    "            print(f\"\\n== decode using {enc} ==\")\n",
    "            print(b.decode(enc))\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to decode with {enc}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example using the function above give you a clear look at a file's byte content and the different ways it might be encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw bytes (hex): efbbbf414e4e45453b4e627265206465207072c3aa74733b54495452453b4155544555523b456469746575723b496e646963653b4249423b434f54453b43617420313b43617420320d0a323031393b3236333b486172727920506f74746572206574206c61206368616d6272652064657320736563726574733b436f6c756d6275732c2043687269733b2d3b5061726973203a205761726e657220486f6d6520566964656f2c20323030332e3b484152523b434142414e49533b462048415252202f323b450d0a32\n",
      "chardet guess: {'encoding': 'UTF-8-SIG', 'confidence': 1.0, 'language': ''}\n",
      "\n",
      "== decode using utf-8 ==\n",
      "﻿ANNEE;Nbre de prêts;TITRE;AUTEUR;Editeur;Indice;BIB;COTE;Cat 1;Cat 2\n",
      "2019;263;Harry Potter et la chambre des secrets;Columbus, Chris;-;Paris : Warner Home Video, 2003.;HARR;CABANIS;F HARR /2;E\n",
      "2\n",
      "\n",
      "== decode using windows-1252 ==\n",
      "ï»¿ANNEE;Nbre de prÃªts;TITRE;AUTEUR;Editeur;Indice;BIB;COTE;Cat 1;Cat 2\n",
      "2019;263;Harry Potter et la chambre des secrets;Columbus, Chris;-;Paris : Warner Home Video, 2003.;HARR;CABANIS;F HARR /2;E\n",
      "2\n",
      "\n",
      "== decode using iso-8859-1 ==\n",
      "ï»¿ANNEE;Nbre de prÃªts;TITRE;AUTEUR;Editeur;Indice;BIB;COTE;Cat 1;Cat 2\n",
      "2019;263;Harry Potter et la chambre des secrets;Columbus, Chris;-;Paris : Warner Home Video, 2003.;HARR;CABANIS;F HARR /2;E\n",
      "2\n",
      "\n",
      "== decode using cp850 ==\n",
      "´╗┐ANNEE;Nbre de pr├¬ts;TITRE;AUTEUR;Editeur;Indice;BIB;COTE;Cat 1;Cat 2\n",
      "2019;263;Harry Potter et la chambre des secrets;Columbus, Chris;-;Paris : Warner Home Video, 2003.;HARR;CABANIS;F HARR /2;E\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "# Example: show encoding diagnostic for a specific file\n",
    "sniff_encoding_and_show(\"top-500-des-films-les-plus-empruntes-a-la-bibliotheque-de-toulouse.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_encoding(filepath: str, sample_bytes: int = 200) -> str:\n",
    "    \"\"\"Guess encoding using chardet from a small sample of the file.\n",
    "    Returns a string encoding; default to 'utf-8' if guess is None.\"\"\"\n",
    "    with open(filepath, \"rb\") as f:\n",
    "        raw = f.read(sample_bytes)\n",
    "    result = chardet.detect(raw)\n",
    "    return result.get(\"encoding\") or \"utf-8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_replacements(val: object, repl: Mapping[str, str]) -> object:\n",
    "    \"\"\"Apply a set of string replacements to correct systematic mojibake\n",
    "    fragments that survived decoding (e.g., 'ãa' -> 'â'). Non-strings left unchanged.\"\"\"\n",
    "    if not isinstance(val, str):\n",
    "        return val\n",
    "    for wrong, right in repl.items():\n",
    "        val = val.replace(wrong, right)\n",
    "    return val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Detection strategy:**\n",
    "  - Use chardet on a bytes sample: gives a best-guess encoding with confidence (`detect_encoding()`).\n",
    "  - Open file using that encoding and `errors=\"replace\"` (see in the later `load_and_clean_csv()` function) to avoid crashes if there are undecodable bytes (this replaces undecodable bytes with �).\n",
    "  - Then apply targeted replacements for known garbled sequences (the `Config.replacements` map is called by `apply_replacements()`). This map is an empiric fix for observed corruptions where some compound sequences map to a single accented character (e.g., `ãa` -> `â`).\n",
    "\n",
    "- Why targeted replacements (not blind re-encoding)? Because:\n",
    "  - Re-decoding already-read text is messy; it's better to open with correct encoding when possible.\n",
    "  - Some mojibake arises from double-encoding (bytes were UTF-8 encoded twice) or from search/replace bugs in the source; targeted fixes are safer than attempting general re-transcoding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Broken alignments: fixing the lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **critical** issue found in the data is row misalignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at these two rows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ANNEE | Nbre de prêts | TITRE | AUTEUR | Editeur | Indice | BIB   | COTE                                 | Cat 1 | Cat 2 |\n",
    "|------|---------------|-------|--------|---------|--------|-------|--------------------------------------|-------|-------|\n",
    "| 2019 | 698           | Okapi | -      | **-**       | **Paris : Bayard presse, 1971-** | **P1468** | **CABANIS** | **P OKAP** | **E** |\n",
    "| 2022 | 404           | Lire  | -      | Paris : Lire, 1975- | P1225 | CABANIS | P LIRE NO. 420 NOV 2013 | A | PERIO |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one (2019) is misaligned. An extra `-` has been inserted in the *Editeur* column, shifting **Paris : Bayard presse, 1971-** (The \"Editeur\" (Publisher)) and latter values one column to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pattern:** In the datasets, some CSV lines have an extra sequence like `;-;` inserted before the last field. This produces extra empty values or shifts columns right when parsed by pandas, causing misalignment. It is also true that in misaligned rows, the last **character** is a known audience category letter (`A/E/B/P`), these letters are valuable *sentinels*.\n",
    "\n",
    "- Why did this happen?\n",
    "Cannot tell for sure. Export scripts sometimes include stray delimiters, escape sequences, or an extra token (\"-\") inserted into the field separator. It can result from manual concatenation, export bugs, or characters that the export tool tried to escape badly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_line(line: str) -> str:\n",
    "    \"\"\"Small heuristic to correct a known misalignment pattern:\n",
    "    some exports include the sequence ';-;' immediately before a trailing Cat1\n",
    "    letter (A/E/B/P). This replaces the first ';-;' with ';' when found and the\n",
    "    last character is in the expected set. This is targeted, not general.\"\"\"\n",
    "    s = line.rstrip()\n",
    "    if s and s[-1] in {\"A\", \"E\", \"B\", \"P\"} and \";-;\" in s:\n",
    "        s = s.replace(\";-;\", \";\", 1)\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- How to fix it :\n",
    "  1. Read the file as raw lines.\n",
    "  2. For each data line (header excluded), check a simple heuristic: if the line ends with one of the expected Cat1 letters (`\"A\", \"E\", \"B\", \"P\"`) and contains the pattern `';-;'`, replace the first `';-;'` with `';'`. This repairs the common pattern that caused a one-column shift.\n",
    "  3. After line repair, parse CSV normally.\n",
    "\n",
    "- Caveats:\n",
    "  - This is a targeted heuristic. **It corrects a concrete, observed bug pattern**. Do not use it blindly for all files.\n",
    "  - If misalignment is caused by quotes or embedded delimiters (e.g., title includes a semicolon not quoted), always prefer fixing the source or using a stricter CSV quoting handler (pandas `read_csv` supports *quotechar* and *escapechar*). For example, if semicolons are legitimately inside quoted fields, ensure the CSV generator quotes them properly; otherwise, write a quoting-aware parser.\n",
    "\n",
    "Reference on [CSV quoting](https://docs.python.org/3/library/csv.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II - Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![A photograph of the Toulouse public Library](img/Médiathèque_José_Cabanis_Toulouse.jpg \"Photographie de l'intérieur de la médiathèque José Cabanis, Toulouse (Source Buffi Associés).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LoanAnalysis Class: overview\n",
    "\n",
    "This Class encapsulates:\n",
    "\n",
    "- Loading and cleaning CSV files (encoding detection, line fixes, characters replacements).\n",
    "- Creating a composite key (title + author) to group identical items.\n",
    "- Filtering on Cat 1 values (cfg.cat1_selector).\n",
    "- Aggregations to compute top-N counts.\n",
    "- Pivot construction for per-year time series for each composite item.\n",
    "- Attaches the Cat 1 value to a record based on the composite key.\n",
    "- Updates a list of records with their corresponding Cat 1 values\n",
    "- Flattens nested lists or dictionaries into a single list of records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoanAnalysis:\n",
    "    def __init__(self, cfg: Config):\n",
    "        self.cfg = cfg\n",
    "        self.cat1_map: Dict[str, str] = {}\n",
    "\n",
    "    def load_and_clean_csv(self, filepath: str, encoding: Optional[str] = None) -> pd.DataFrame:\n",
    "        enc = encoding or detect_encoding(filepath)\n",
    "        # Read raw lines so we can apply line-level fixes before parsing CSV\n",
    "        with open(filepath, \"r\", encoding=enc, errors=\"replace\") as fin:\n",
    "            lines = fin.readlines()\n",
    "        if not lines:\n",
    "            raise ValueError(\"Empty CSV file\")\n",
    "\n",
    "        # Keep header as-is; fix only subsequent lines (targeted repair)\n",
    "        header = lines[0].rstrip(\"\\n\\r\")\n",
    "        data_lines = [fix_line(l) for l in lines[1:]]\n",
    "        unified = StringIO(\"\\n\".join([header] + data_lines))\n",
    "\n",
    "        # Use pandas to parse; delimiter is ';' for the CSVs\n",
    "        df = pd.read_csv(\n",
    "            unified,\n",
    "            delimiter=\";\",\n",
    "            engine=\"python\",\n",
    "            encoding=enc,\n",
    "            encoding_errors=\"replace\"\n",
    "        )\n",
    "\n",
    "        # Apply character replacements on all object columns\n",
    "        for c in df.select_dtypes(include=[\"object\"]).columns:\n",
    "            df[c] = df[c].apply(lambda v: apply_replacements(v, self.cfg.replacements))\n",
    "\n",
    "        # Normalize numerical columns with safe coercion\n",
    "        df[self.cfg.year_field] = pd.to_numeric(df[self.cfg.year_field], errors=\"coerce\").astype(\"Int64\")\n",
    "        df[self.cfg.popularity_field] = pd.to_numeric(df[self.cfg.popularity_field], errors=\"coerce\").astype(\"Int64\")\n",
    "\n",
    "        # Composite key for items: title + author\n",
    "        df[self.cfg.composite_field] = (\n",
    "            df[self.cfg.title_field].astype(str)\n",
    "            + \" --- \"\n",
    "            + df[self.cfg.author_field].astype(str)\n",
    "        )\n",
    "\n",
    "        # Build mapping from composite key -> Cat1 (join-back later)\n",
    "        grp = df.groupby(self.cfg.composite_field)[self.cfg.cat1_field].unique()\n",
    "        self.cat1_map = {\n",
    "            k: (\", \".join(sorted(v)) if len(v) > 1 else v[0])\n",
    "            for k, v in grp.to_dict().items()\n",
    "        }\n",
    "\n",
    "        # Filter rows by Cat 1 allowed values (cfg.cat1_selector)\n",
    "        df = df[df[self.cfg.cat1_field].isin(self.cfg.cat1_selector)]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def aggregate_by(self, df: pd.DataFrame, groupby_fields: Iterable[str], key_field: str,\n",
    "                     value_field: str) -> pd.DataFrame:\n",
    "        fields = list(groupby_fields) + [key_field]\n",
    "        return df.groupby(fields, as_index=False)[value_field].sum()\n",
    "\n",
    "    def top_n_from_agg(self, agg_df: pd.DataFrame, value_field: str, top_n: int) -> pd.DataFrame:\n",
    "        return agg_df.sort_values(value_field, ascending=False).head(top_n)\n",
    "\n",
    "    def group_top_n(self, df: pd.DataFrame, groupby_fields: Iterable[str],\n",
    "                    key_field: str, value_field: str, top_n: int) -> Dict[str, List[dict]]:\n",
    "        agg = self.aggregate_by(df, groupby_fields, key_field, value_field)\n",
    "        result: Dict[str, List[dict]] = {}\n",
    "        gb = list(groupby_fields)\n",
    "        for group_vals, sub in agg.groupby(gb):\n",
    "            key = \", \".join(map(str, group_vals)) if isinstance(group_vals, tuple) else str(group_vals)\n",
    "            result[key] = self.top_n_from_agg(sub, value_field, top_n).to_dict(orient=\"records\")\n",
    "        return result\n",
    "\n",
    "    def _build_pivot(self, df: pd.DataFrame, index: str, columns: str, values: str) -> pd.DataFrame:\n",
    "        return df.pivot_table(index=index, columns=columns, values=values, aggfunc=\"sum\", fill_value=0)\n",
    "\n",
    "    def attach_cat1(self, record: dict) -> dict:\n",
    "        item = record.get(self.cfg.composite_field)\n",
    "        if item in self.cat1_map:\n",
    "            record[self.cfg.cat1_field] = self.cat1_map[item]\n",
    "        return record\n",
    "\n",
    "    def update_records_with_cat(self, records: Iterable[dict]) -> List[dict]:\n",
    "        return [self.attach_cat1(dict(r)) for r in records]\n",
    "\n",
    "    @staticmethod\n",
    "    def flatten_top_lists(input_data) -> List[dict]:\n",
    "        flat: List[dict] = []\n",
    "        if isinstance(input_data, list):\n",
    "            flat.extend(input_data)\n",
    "        elif isinstance(input_data, dict):\n",
    "            for v in input_data.values():\n",
    "                if isinstance(v, list):\n",
    "                    flat.extend(v)\n",
    "                elif isinstance(v, dict):\n",
    "                    flat.extend(LoanAnalysis.flatten_top_lists(v))\n",
    "        return flat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trending and shooting star logics\n",
    "\n",
    "1) **Trending:** finding items whose counts change between their first and last non-zero years in a year-by-year pivot table.\n",
    "\n",
    "**Steps:**\n",
    "- `compute_trends()` constructs a pivot table: index = composite item, columns = years, values = loan counts.\n",
    "- It filters out items that appear in fewer than two non zero years (`pivot_df > 0).sum(axis=1) >= 2`) because progression/regression requires at least two data points.\n",
    "- For each item, it finds the first and last year with non-zero loans. It then compares loan counts in those years: diff = last - first.\n",
    "  - progression: `diff > 0` (increasing popularity).\n",
    "  - regression: `diff < 0` (decreasing popularity).\n",
    "  \n",
    "- `compute_trend_by_group()` splits a DataFrame by a grouping field, builds a pivot for each group, calls `compute_trends` and collects the per-group trend results as lists of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_trends(self, pivot_df: pd.DataFrame, trend: str = \"progression\", top_n: Optional[int] = None) -> pd.DataFrame:\n",
    "    top_n = top_n or self.cfg.top_n\n",
    "    mask = (pivot_df > 0).sum(axis=1) >= 2\n",
    "    pivot_filtered = pivot_df[mask].copy()\n",
    "    if pivot_filtered.empty:\n",
    "        return pd.DataFrame(columns=[self.cfg.composite_field, \"first_year\", \"last_year\", \"sum_first\", \"sum_last\", \"diff\"])\n",
    "\n",
    "    # first and last non-zero year (columns are years; ensure they're ints)\n",
    "    first_year = pivot_filtered.apply(lambda r: int(r[r > 0].index.min()), axis=1)\n",
    "    last_year = pivot_filtered.apply(lambda r: int(r[r > 0].index.max()), axis=1)\n",
    "\n",
    "    sum_first = []\n",
    "    sum_last = []\n",
    "    for idx, fy, ly in zip(pivot_filtered.index, first_year, last_year):\n",
    "        sum_first.append(int(pivot_filtered.at[idx, fy]))\n",
    "        sum_last.append(int(pivot_filtered.at[idx, ly]))\n",
    "\n",
    "    df_trend = pd.DataFrame({\n",
    "        self.cfg.composite_field: pivot_filtered.index,\n",
    "        \"first_year\": first_year.values,\n",
    "        \"last_year\": last_year.values,\n",
    "        \"sum_first\": sum_first,\n",
    "        \"sum_last\": sum_last,\n",
    "        \"diff\": [s_l - s_f for s_f, s_l in zip(sum_first, sum_last)]\n",
    "    }).reset_index(drop=True)\n",
    "\n",
    "    if trend == \"progression\":\n",
    "        return df_trend[df_trend[\"diff\"] > 0].sort_values(\"diff\", ascending=False).head(top_n)\n",
    "    if trend == \"regression\":\n",
    "        return df_trend[df_trend[\"diff\"] < 0].sort_values(\"diff\", ascending=True).head(top_n)\n",
    "    raise ValueError(\"trend must be 'progression' or 'regression'.\")\n",
    "\n",
    "def compute_trend_by_group(self, df: pd.DataFrame, group_field: str, value_field: str, trend: str) -> Dict[str, List[dict]]:\n",
    "    result: Dict[str, List[dict]] = {}\n",
    "    for grp, sub in df.groupby(group_field):\n",
    "        pivot = self._build_pivot(sub, self.cfg.composite_field, self.cfg.year_field, value_field)\n",
    "        trend_df = self.compute_trends(pivot, trend=trend)\n",
    "        for col in (\"sum_first\", \"sum_last\", \"diff\"):\n",
    "            if col in trend_df:\n",
    "                trend_df[col] = trend_df[col].astype(int)\n",
    "        result[str(grp)] = trend_df.to_dict(orient=\"records\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) **Disappearing:** detect items that suddenly drop to zero the year after their last recorded non-zero year in a year-by-year pivot table.\n",
    "   \n",
    "**Steps:**\n",
    "\n",
    "- `compute_sudden_disappearances()` iterates each item, finds its non-zero years, takes the last such year, and if the immediate next year exists in the columns with a zero value it records the item, that last year, and the value in that last year. \n",
    "\n",
    "*In short:* finds items with loans in some year Y and zero loans in Y+1 (explicitly checks existence of Y+1 column). This is a pragmatic definition of a shooting star.\n",
    "- `compute_disappearances_by_group()` groups the input DataFrame, builds a year pivot for each group, runs the disappearance detector, and returns per-group lists of disappearance records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sudden_disappearances(self, pivot_df: pd.DataFrame, top_n: Optional[int] = None) -> pd.DataFrame:\n",
    "    top_n = top_n or self.cfg.top_n\n",
    "    records = []\n",
    "    for item in pivot_df.index:\n",
    "        # columns are years; find non-zero years for this item\n",
    "        nonzero = pivot_df.columns[pivot_df.loc[item] > 0]\n",
    "        if len(nonzero) == 0:\n",
    "            continue\n",
    "        last_year = nonzero.max()\n",
    "        # check if next year exists and is zero (sudden disappearance)\n",
    "        if (last_year + 1) in pivot_df.columns and pivot_df.loc[item, last_year + 1] == 0:\n",
    "            records.append({\n",
    "                self.cfg.composite_field: item,\n",
    "                \"last_year\": int(last_year),\n",
    "                \"loan_last\": int(pivot_df.loc[item, last_year])\n",
    "            })\n",
    "    if not records:\n",
    "        return pd.DataFrame(columns=[self.cfg.composite_field, \"last_year\", \"loan_last\"])\n",
    "    return pd.DataFrame(records).sort_values(\"loan_last\", ascending=False).head(top_n)\n",
    "\n",
    "def compute_disappearances_by_group(self, df: pd.DataFrame, group_field: str, value_field: str) -> Dict[str, List[dict]]:\n",
    "    result: Dict[str, List[dict]] = {}\n",
    "    for grp, sub in df.groupby(group_field):\n",
    "        pivot = self._build_pivot(sub, self.cfg.composite_field, self.cfg.year_field, value_field)\n",
    "        df_disp = self.compute_sudden_disappearances(pivot)\n",
    "        result[str(grp)] = df_disp.to_dict(orient=\"records\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `run_all()` builds a comprehensive report of \"top\" lists and trend analyses from a DataFrame. It:\n",
    "\n",
    "  - Aggregates overall popularity and returns the top N overall.\n",
    "  - Computes top N per year (merged across audiences) and top N separated by audience type.\n",
    "  - Counts which titles appear most often among yearly top-N lists (overall and per audience).\n",
    "  - Computes progressions and regressions per audience and overall by building a year-by-year pivot and using the trend functions.\n",
    "  - Detects shooting stars (sudden disappearances) per audience and overall.\n",
    "  - Tallies nominations across many result lists to produce the most frequently nominated titles.\n",
    "\n",
    "- Returns a dictionary R whose keys describe each result section; each value contains a short question string and the resulting records (lists or dicts) ready for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all(self, df: pd.DataFrame) -> Dict[str, dict]:\n",
    "    R = {}\n",
    "    C = self.cfg\n",
    "\n",
    "    # topN overall across all years (aggregated)\n",
    "    agg_all = self.aggregate_by(df, [], C.composite_field, C.popularity_field)\n",
    "    topN_all = self.top_n_from_agg(agg_all, C.popularity_field, C.top_n).to_dict(orient=\"records\")\n",
    "    R[\"topN_all_years_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} most popular items overall\",\n",
    "        \"result\": self.update_records_with_cat(topN_all)\n",
    "    }\n",
    "\n",
    "    # Top per year (merged across Cat1)\n",
    "    agg_year = self.aggregate_by(df, [C.year_field], C.composite_field, C.popularity_field)\n",
    "    by_year: Dict[int, List[dict]] = {}\n",
    "    for yr, sub in agg_year.groupby(C.year_field):\n",
    "        recs = self.top_n_from_agg(sub, C.popularity_field, C.top_n).to_dict(orient=\"records\")\n",
    "        by_year[int(yr)] = self.update_records_with_cat(recs)\n",
    "    R[\"topN_by_year_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} most popular items per year, all audiences merged\",\n",
    "        \"result\": by_year\n",
    "    }\n",
    "\n",
    "    # Tops separated by Cat1\n",
    "    R[\"topN_all_years_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} most popular items across all years, categorized by audience (A: adults, E: kids)\",\n",
    "        \"result\": self.group_top_n(df, [C.cat1_field], C.composite_field, C.popularity_field, C.top_n)\n",
    "    }\n",
    "    R[\"topN_by_year_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} most popular items per year, categorized by audience (A: adults, E:kids)\",\n",
    "        \"result\": self.group_top_n(df, [C.year_field, C.cat1_field], C.composite_field, C.popularity_field, C.top_n)\n",
    "    }\n",
    "\n",
    "    # Titles appearing most often across yearly topNs (merged)\n",
    "    merged_counter = Counter()\n",
    "    for recs in R[\"topN_by_year_all_cat1\"][\"result\"].values():\n",
    "        for rec in recs:\n",
    "            merged_counter[rec[C.composite_field]] += 1\n",
    "    R[\"topN_titles_occurring_most_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} titles that appear most frequently among top tens (all audiences)\",\n",
    "        \"result\": [\n",
    "            {C.composite_field: k, \"appearances\": v, C.cat1_field: self.cat1_map.get(k, \"\")}\n",
    "            for k, v in merged_counter.most_common(C.top_n)\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # Titles appearing most often per Cat1\n",
    "    temp_counter: Dict[str, Counter] = defaultdict(Counter)\n",
    "    for group_key, recs in R[\"topN_by_year_separated_cat1\"][\"result\"].items():\n",
    "        cat = group_key.split(\",\")[-1].strip() if \",\" in group_key else group_key\n",
    "        for rec in recs:\n",
    "            temp_counter[cat][rec[C.composite_field]] += 1\n",
    "\n",
    "    sep_occ: Dict[str, List[dict]] = {}\n",
    "    for cat, counter in temp_counter.items():\n",
    "        sep_occ[cat] = [\n",
    "            {C.composite_field: k, \"appearances\": v, C.cat1_field: cat}\n",
    "            for k, v in counter.most_common(C.top_n)\n",
    "        ]\n",
    "    R[\"topN_titles_occurring_most_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} titles that appear most frequently among top tens (categorized by audiences)\",\n",
    "        \"result\": sep_occ\n",
    "    }\n",
    "\n",
    "    # Progressions/regressions and disappearances\n",
    "    R[\"topN_progressions_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} progressions, categorized by audience\",\n",
    "        \"result\": self.compute_trend_by_group(df, C.cat1_field, C.popularity_field, trend=\"progression\")\n",
    "    }\n",
    "\n",
    "    pivot_all = self._build_pivot(df, C.composite_field, C.year_field, C.popularity_field)\n",
    "    prog_all = self.compute_trends(pivot_all, trend=\"progression\")\n",
    "    prog_all = prog_all.astype({\"sum_first\": int, \"sum_last\": int, \"diff\": int})\n",
    "    R[\"topN_progressions_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} progressions, overall\",\n",
    "        \"result\": self.update_records_with_cat(prog_all.to_dict(orient=\"records\"))\n",
    "    }\n",
    "\n",
    "    R[\"topN_regressions_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n}, downward trend, categorized by audience\",\n",
    "        \"result\": self.compute_trend_by_group(df, C.cat1_field, C.popularity_field, trend=\"regression\")\n",
    "    }\n",
    "\n",
    "    reg_all = self.compute_trends(pivot_all, trend=\"regression\")\n",
    "    reg_all = reg_all.astype({\"sum_first\": int, \"sum_last\": int, \"diff\": int})\n",
    "    R[\"topN_regressions_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} downward trend, overall\",\n",
    "        \"result\": self.update_records_with_cat(reg_all.to_dict(orient=\"records\"))\n",
    "    }\n",
    "\n",
    "    R[\"topN_sudden_disappearances_separated_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} shooting stars categorized by audience\",\n",
    "        \"result\": self.compute_disappearances_by_group(df, C.cat1_field, C.popularity_field)\n",
    "    }\n",
    "\n",
    "    disp_all = self.compute_sudden_disappearances(pivot_all)\n",
    "    R[\"topN_sudden_disappearances_all_cat1\"] = {\n",
    "        \"question\": f\"Top {C.top_n} shooting stars, overall\",\n",
    "        \"result\": self.update_records_with_cat(disp_all.to_dict(orient=\"records\"))\n",
    "    }\n",
    "\n",
    "    # Nomination counting across many result lists to find frequently nominated titles\n",
    "    keys = [\n",
    "        \"topN_all_years_all_cat1\",\n",
    "        \"topN_by_year_all_cat1\",\n",
    "        \"topN_all_years_separated_cat1\",\n",
    "        \"topN_by_year_separated_cat1\",\n",
    "        \"topN_sudden_disappearances_all_cat1\",\n",
    "        \"topN_sudden_disappearances_separated_cat1\",\n",
    "        \"topN_titles_occurring_most_separated_cat1\",\n",
    "        \"topN_progressions_all_cat1\",\n",
    "        \"topN_progressions_separated_cat1\",\n",
    "        \"topN_regressions_all_cat1\",\n",
    "        \"topN_regressions_separated_cat1\"\n",
    "    ]\n",
    "    nom_counter = Counter()\n",
    "    for k in keys:\n",
    "        data = R.get(k, {}).get(\"result\", {})\n",
    "        for rec in self.flatten_top_lists(data):\n",
    "            nom_counter[rec[C.composite_field]] += 1\n",
    "\n",
    "    topN_nominated = [\n",
    "        {C.composite_field: k, \"nominations\": v, C.cat1_field: self.cat1_map.get(k, \"\")}\n",
    "        for k, v in nom_counter.most_common(C.top_n)\n",
    "    ]\n",
    "    R[\"topN_most_nominated_entries\"] = {\n",
    "        \"question\": f\"Top {C.top_n} most nominated entries\",\n",
    "        \"result\": topN_nominated\n",
    "    }\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach orphan functions to LoanAnalysis\n",
    "LoanAnalysis.compute_trends = compute_trends\n",
    "LoanAnalysis.compute_sudden_disappearances = compute_sudden_disappearances\n",
    "LoanAnalysis.compute_trend_by_group = compute_trend_by_group\n",
    "LoanAnalysis.compute_disappearances_by_group = compute_disappearances_by_group\n",
    "LoanAnalysis.run_all = run_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Re-creates a Config for each discriminator (so output file names differ).\n",
    "- `detect_encoding()` is called before reading to make an informed choice.\n",
    "- Outputs are saved with UTF-8 to ensure consistent JSON encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(cfg: Optional[Config] = None) -> None:\n",
    "    cfg = cfg or Config()\n",
    "    for disc in cfg.discriminator_list:\n",
    "        local_cfg = Config(\n",
    "            dataset_folder=cfg.dataset_folder,\n",
    "            output_folder=cfg.output_folder,\n",
    "            discriminator=disc,\n",
    "            discriminator_list=cfg.discriminator_list,\n",
    "            top_n=cfg.top_n,\n",
    "            year_field=cfg.year_field,\n",
    "            popularity_field=cfg.popularity_field,\n",
    "            title_field=cfg.title_field,\n",
    "            author_field=cfg.author_field,\n",
    "            cat1_field=cfg.cat1_field,\n",
    "            cat1_selector=cfg.cat1_selector,\n",
    "            composite_field=cfg.composite_field,\n",
    "            replacements=cfg.replacements\n",
    "        )\n",
    "\n",
    "        pattern = f\"*{local_cfg.discriminator}*.csv\"\n",
    "        try:\n",
    "            filepath = find_first_matching_file(os.path.join(local_cfg.dataset_folder, pattern))\n",
    "        except FileNotFoundError:\n",
    "            print(f\"No CSV found for discriminator {local_cfg.discriminator!r} (pattern: {pattern}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        enc = detect_encoding(filepath)\n",
    "        print(f\"[{local_cfg.discriminator}] Detected encoding: {enc}\")\n",
    "\n",
    "        analysis = LoanAnalysis(local_cfg)\n",
    "        df = analysis.load_and_clean_csv(filepath, encoding=enc)\n",
    "        \n",
    "        results = analysis.run_all(df)\n",
    "\n",
    "        output_filepath = os.path.join(local_cfg.output_folder, \"results_\" + local_cfg.discriminator + \".json\")\n",
    "        # Create output folder if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_filepath), exist_ok=True)\n",
    "        with open(output_filepath, \"w\", encoding=\"utf-8\") as fout:\n",
    "            json.dump(results, fout, ensure_ascii=False, indent=4)\n",
    "        print(f\"Wrote results to {output_filepath}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[films] Detected encoding: UTF-8-SIG\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote results to output/results_films.json\n",
      "[imprimes] Detected encoding: UTF-8-SIG\n",
      "Wrote results to output/results_imprimes.json\n",
      "[cds] Detected encoding: UTF-8-SIG\n",
      "Wrote results to output/results_cds.json\n"
     ]
    }
   ],
   "source": [
    "# Run the full pipeline\n",
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- [chardet](https://pypi.org/project/chardet/)\n",
    "- [Unicode, encodings and mojibake (Joel Spolsky)](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n",
    "- [Mojibake (Wikipedia)](https://en.wikipedia.org/wiki/Mojibake)\n",
    "- [Python csv module](https://docs.python.org/3/library/csv.html)\n",
    "- [pandas read_csv docs](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html) (useful parameters: encoding, sep, delimiter, quotechar, escapechar, engine): \n",
    "\n",
    "Big thanks to the [Toulouse métropole open data team](https://data.toulouse-metropole.fr/page/opendata/) and the library webmaster for sharing a dataset that reveals Toulouse’s cultural heartbeat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** if you prefer not to use this interactive jupyter notebook, you can run the same code as a standalone Python script. The standalone script is included in the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Outside photographs of the Toulouse public Library](img/toulouse_public_library.jpg \"Photographies de l'intérieur de la médiathèque José Cabanis, Toulouse (Source Buffi Associés).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Facts about the Toulouse main public library:**\n",
    "\n",
    "Médiathèque José Cabanis\n",
    "\n",
    "The building features a blend of modern, austere lines and various materials, combining the warmth of terracotta and wood with the brightness of glass and metal. Its forecourt, designed by Guy de Rougemont, consists of gray, black, white, and pink granite slabs. The rear façade is brick-clad, while the front is primarily glass, wood, accented by terracotta sunshades that provide protection without blocking views. These panels also regulate temperature and according to Jean-Pierre Buffi, they also create a light filter for the more intimate interior spaces, echoing inside the city's red brick architecture.\n",
    "\n",
    "Inside, a vertical shaft spans the first four floors and basement, enhancing the building's height. A large spiral staircase made of metal blades connects the levels, emphasizing verticality. Public areas are organized around a central void that offers panoramic views, complemented by a panoramic elevator showcasing the district's tiled roofs. The roof, designed as a mesh, serves as a fifth façade visible from Jolimont Hill. The garden level opens to a landscaped English courtyard with brick slabs arranged in a herringbone pattern.\n",
    "\n",
    "- Médiathèque/auditorium : 13 500 m2 \n",
    "- Institut national de l'audiovisuel (INA) : 1000 m2 \n",
    "- Reception spaces : 2500 m2 \n",
    "- Cafés and shops : 1300 m2 \n",
    "- Local TV 'TLT : 1300 m2 \n",
    "- Parking and train/metro access: 2000 m2\n",
    "\n",
    "  - Total surface : 25 500 m2 (I know, it's not adding up -no idea how it's calculated :)\n",
    "\n",
    "    - Finished building on april 2004 after 6 years of works.\n",
    "    - Costs: 35,02 M€\n",
    "    - Architects : Jean-Pierre et Marianne Buffi avec F. E. Greteau et C. Ramin / associates : Séquences Architectes, J. Hurtevent et P. Laborderie\n",
    "\n",
    "**Opening hours** (find me wandering there at times!):\n",
    "\n",
    "  - Tuesday: 10am–7pm\n",
    "  - Wednesday: 10am–7pm\n",
    "  - Thursday: 2pm–7pm\n",
    "  - Friday: 2pm–7pm\n",
    "  - Saturday: 10am–7pm\n",
    "  - Sunday: 2pm–6pm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Completed in september 2025 by [brk♛](github.com/brooks-code).\n",
    "\n",
    "License: [\"CC0 - No rights reserved\"](https://creativecommons.org/public-domain/cc0/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
